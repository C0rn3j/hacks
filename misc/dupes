#!/usr/bin/env python
# Released under WTFPL v2 <http://sam.zoy.org/wtfpl/>

import os
import sys
import stat
import binascii
import hashlib
import math
from collections import defaultdict
from optparse import OptionParser

# this doesn't need to be declared here
# I'm just doing so as a reminder that `opts` is global
opts = None
_isatty = None
_ttywidth = None

# header and hash caches, to avoid reading
# or hashing the same file twice
header_size = 512
file_sizes = {}     # path → size
file_headers = {}   # path → header
file_hashes = {}    # path → hash
total_wasted = 0

def isatty():
    global _isatty
    if _isatty is None:
        _isatty = sys.stderr.isatty()
    return _isatty

def ttywidth():
    global _ttywidth
    if _ttywidth is None:
        with os.popen("stty size", "r") as fh:
            line = fh.read().strip()
        rows, cols = line.split()
        _ttywidth = int(cols)
    return _ttywidth

def status(*args):
    if isatty() and not opts.verbose:
        msg = " ".join(args)
        msg = msg.replace("\n", " ")
        msg = msg[:ttywidth()]
        sys.stderr.write("\r\033[K\033[33m%s\033[m" % msg)
        sys.stderr.flush()

def weed_ignores(dirs):
    ignores = {".git", ".hg"}
    for item in dirs[:]:
        if item in ignores:
            dirs.remove(item)

def enum_files(root_dir):
    for subdir, dirs, files in os.walk(root_dir):
        weed_ignores(dirs)
        for name in files:
            path = os.path.join(subdir, name)
            yield path

def get_header(path):
    if path not in file_headers:
        if opts.verbose:
            print("reading", path, file=sys.stderr)
        with open(path, "rb") as fh:
            file_headers[path] = fh.read(header_size)
    return file_headers[path]

def hash_file(path):
    if path not in file_hashes:
        if opts.verbose:
            print("hashing", path, file=sys.stderr)
        h = hashlib.sha1()
        with open(path, "rb") as fh:
            buf = True
            while buf:
                buf = fh.read(4194304)
                h.update(buf)
        file_hashes[path] = h.digest()
    return file_hashes[path]

def fmt_hash(hash):
    return binascii.b2a_hex(hash).decode("utf-8")

def fmt_size(nbytes, si=False):
    if nbytes == 0:
        return "zero bytes"
    prefixes = ".kMGTPE"
    div = 1000 if si else 1024
    exp = int(math.log(nbytes, div))
    if exp == 0:
        return "%.1f bytes" % nbytes
    elif exp < len(prefixes):
        quot = nbytes / div**exp
        return "%.1f %sB" % (quot, prefixes[exp])
    else:
        exp = len(prefixes) - 1
        quot = nbytes / div**exp
        return "%f %sB" % (quot, prefixes[exp])
    return str(nbytes)

def find_duplicates(root_dirs):
    # dicts keeping duplicate items
    known_sizes = defaultdict(list)     # size → path[]
    known_headers = defaultdict(list)   # (size, header) → path[]
    known_hashes = defaultdict(list)    # (size, hash) → path[]

    n_size = 0
    n_head = 0
    n_hash = 0

    # find files identical in size
    for root_dir in root_dirs:
        for path in enum_files(root_dir):
            n_size += 1
            status("stat (%d)" % n_size, path)
            st = os.lstat(path)
            if not stat.S_ISREG(st.st_mode):
                continue
            file_sizes[path] = st.st_size
            known_sizes[st.st_size].append(path)

    status()

    # find files identical in size and first `header_size` bytes
    for size, paths in known_sizes.items():
        if len(paths) < 2:
            continue

        for path in paths:
            n_head += 1
            status("head (%d/%d)" % (n_head, n_size), path)
            header = get_header(path)
            known_headers[size, header].append(path)

    status()

    # find files identical in size and hash
    for (size, header), paths in known_headers.items():
        if len(paths) < 2:
            continue
        if size <= header_size:
            # optimization: don't compare by hash if
            # the entire contents are already known
            status()
            yield paths
            continue

        for path in paths:
            n_hash += 1
            status("hash (%d/%d)" % (n_hash, n_head), path)
            filehash = hash_file(path)
            known_hashes[size, filehash].append(path)

    status()

    for (size, filehash), paths in known_hashes.items():
        if len(paths) < 2:
            continue
        yield paths

cli_usage = "%prog [options] {path}"

cli_desc = """\
Finds files with duplicate data.
"""

cli_epilog = """\
This program ignores symlinks, special files, and the like. It also does not know about hardlinks; this might be added as an optimization later.
"""

cli_version = "1.∞"

if __name__ == "__main__":
    op = OptionParser(
        usage=cli_usage,
        description=cli_desc,
        epilog=cli_epilog,
        version=cli_version)
    op.add_option("-v", "--verbose", dest="verbose", action="store_true", default=False,
        help="show files as they are processed")
    op.add_option("-l", "--list", dest="list", action="store_true", default=False,
        help="output files as a sortable list")

    opts, args = op.parse_args()
    if args:
        root_dir = args[:]
    else:
        root_dir = ["."]

    try:
        for paths in find_duplicates(root_dir):
            size = file_sizes[paths[0]]
            hash = hash_file(paths[0])
            wasted = size * (len(paths) - 1)
            if opts.list:
                for path in paths:
                    print(wasted, fmt_hash(hash), path)
            else:
                print("Duplicates (%s wasted):" % fmt_size(wasted))
                for path in paths:
                    print("    ", path)
            total_wasted += wasted
    except KeyboardInterrupt:
        status()
        print("Interrupted.")

    if opts.verbose or opts.list:
        print("; %d files compared by header" % len(file_headers))
        print("; %d files compared by hash" % len(file_hashes))
        print("; %s wasted by duplicates" % fmt_size(total_wasted))
    else:
        print("Total %s wasted." % fmt_size(total_wasted))
